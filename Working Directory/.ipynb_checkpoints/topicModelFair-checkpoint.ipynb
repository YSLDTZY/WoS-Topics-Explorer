{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNLemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('4770WoSarticles.txt', 'r')\n",
    "paraCount = 4770\n",
    "docs = []\n",
    "for i in range(int(4000)):\n",
    "    doc = f.readline().rstrip()\n",
    "    doc = nltk.word_tokenize(doc)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop1 = ['model', 'system', 'paper', 'research', 'study', 'result', 'method', 'approach', 'design', 'problem']\n",
    "stop2 = ['value', 'relationship', 'effect', 'cause', 'function', 'analysis']\n",
    "\n",
    "stopWords2 = stop1 + stop2\n",
    "stopWords2 += stopwords.words('english')\n",
    "\n",
    "for i in range(len(stopWords)):\n",
    "    stopWords[i] = WNLemma.lemmatize(stopWords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(docs)):\n",
    "    doc_tokens = docs[i]\n",
    "    newTokens = []\n",
    "    for j in range(len(doc_tokens)):\n",
    "        token = doc_tokens[j]\n",
    "        if token.istitle():\n",
    "            token = token.lower()\n",
    "        token = WNLemma.lemmatize(token)\n",
    "        tag = nltk.pos_tag([token])[0][1]\n",
    "        if (token not in stopWords) and len(token) > 3:\n",
    "            newTokens.append(token)\n",
    "    docs[i] = newTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35362 unique tokens: ['MANOVA', 'appeared', 'asked', 'assessed', 'based']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=0.1, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 35359\n",
      "Number of documents: 4000\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.020*\"were\" + 0.015*\"patient\" + 0.011*\"care\" + 0.009*\"nurse\" + '\n",
      "  '0.008*\"group\" + 0.007*\"student\" + 0.007*\"health\" + 0.007*\"their\" + '\n",
      "  '0.007*\"nursing\" + 0.007*\"from\"'),\n",
      " (1,\n",
      "  '0.009*\"from\" + 0.009*\"their\" + 0.006*\"have\" + 0.006*\"which\" + 0.006*\"these\" '\n",
      "  '+ 0.005*\"between\" + 0.005*\"student\" + 0.005*\"learning\" + 0.005*\"more\" + '\n",
      "  '0.004*\"teacher\"'),\n",
      " (2,\n",
      "  '0.013*\"equation\" + 0.012*\"elsevier\" + 0.012*\"right\" + 0.012*\"solution\" + '\n",
      "  '0.012*\"reserved\" + 0.008*\"element\" + 0.008*\"some\" + 0.007*\"space\" + '\n",
      "  '0.006*\"condition\" + 0.006*\"inc.\"'),\n",
      " (3,\n",
      "  '0.012*\"flow\" + 0.007*\"pressure\" + 0.006*\"surface\" + 0.006*\"were\" + '\n",
      "  '0.005*\"used\" + 0.005*\"wing\" + 0.005*\"dynamic\" + 0.005*\"condition\" + '\n",
      "  '0.004*\"angle\" + 0.004*\"structure\"'),\n",
      " (4,\n",
      "  '0.004*\"food\" + 0.004*\"coverage\" + 0.003*\"mentoring\" + 0.003*\"climate\" + '\n",
      "  '0.003*\"farmer\" + 0.003*\"text\" + 0.003*\"farm\" + 0.003*\"public\" + '\n",
      "  '0.003*\"gender\" + 0.002*\"crop\"'),\n",
      " (5,\n",
      "  '0.011*\"proposed\" + 0.010*\"algorithm\" + 0.008*\"based\" + 0.008*\"which\" + '\n",
      "  '0.007*\"elsevier\" + 0.006*\"using\" + 0.006*\"data\" + 0.006*\"from\" + '\n",
      "  '0.006*\"performance\" + 0.006*\"reserved\"'),\n",
      " (6,\n",
      "  '0.024*\"cell\" + 0.014*\"compound\" + 0.014*\"activity\" + 0.010*\"were\" + '\n",
      "  '0.008*\"cancer\" + 0.008*\"protein\" + 0.007*\"elsevier\" + 0.007*\"right\" + '\n",
      "  '0.007*\"reserved\" + 0.007*\"against\"'),\n",
      " (7,\n",
      "  '0.006*\"beta\" + 0.005*\"alpha\" + 0.004*\"flavonoid\" + 0.003*\"analyzer\" + '\n",
      "  '0.003*\"compliance\" + 0.003*\"mirror\" + 0.002*\"spike\" + 0.002*\"milk\" + '\n",
      "  '0.002*\"plagiarism\" + 0.002*\"saponin\"'),\n",
      " (8,\n",
      "  '0.015*\"were\" + 0.008*\"from\" + 0.007*\"detection\" + 0.006*\"using\" + '\n",
      "  '0.005*\"surface\" + 0.005*\"high\" + 0.005*\"which\" + 0.004*\"property\" + '\n",
      "  '0.004*\"concentration\" + 0.004*\"drug\"'),\n",
      " (9,\n",
      "  '0.007*\"phase\" + 0.004*\"material\" + 0.003*\"pore\" + 0.003*\"after\" + '\n",
      "  '0.003*\"ca2+\" + 0.003*\"diamond\" + 0.003*\"cardiac\" + 0.003*\"pressure\" + '\n",
      "  '0.003*\"pulse\" + 0.003*\"materialia\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(model.print_topics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
