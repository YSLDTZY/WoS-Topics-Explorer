{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNLemma = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if adding these stopwords improves accuracy\n",
    "# stopwords = nltk.corpus.stopwords\n",
    "# stop_words = list(set(stopwords.words('english')))\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_stopwords = ['.', ',', 'studies', 'research', 'article', 'examines', 'investigates', 'papers', 'presents', 'argues', 'surveys', 'gaps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(context_stopwords)):\n",
    "#     word = context_stopwords[i]\n",
    "#     word = WNLemma.lemmatize(word)\n",
    "#     stop_words.append(word)\n",
    "# stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('4770WoSarticles.txt', 'r')\n",
    "paraCount = 4770\n",
    "docs = []\n",
    "for i in range(int(paraCount/2)):\n",
    "    doc = f.readline().rstrip()\n",
    "    doc = nltk.word_tokenize(doc)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studied'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNLemma.lemmatize('studied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(['bed'])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses indices to modify the array in-place\n",
    "# It removes stopwords before lemmatizing each word\n",
    "\n",
    "# for i in range(len(docs)):\n",
    "#     doc_tokens = docs[i]\n",
    "#     filtered_tokens = []\n",
    "#     for word in doc_tokens:\n",
    "#         if word.istitle():\n",
    "#             word = word.lower()\n",
    "#         if word not in stop_words: \n",
    "#             filtered_tokens.append(word) \n",
    "#         for j in range(len(filtered_tokens)):\n",
    "#             token = filtered_tokens[j]\n",
    "#             filtered_tokens[j] = WNLemma.lemmatize(token)\n",
    "#             all.append(filtered_tokens[j])\n",
    "#     docs[i] = list(set(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(docs)):\n",
    "#     doc_tokens = docs[i]\n",
    "#     newTokens = []\n",
    "#     for j in range(len(doc_tokens)):\n",
    "#         token = doc_tokens[j]\n",
    "#         if token.istitle():\n",
    "#             token = token.lower()\n",
    "#         token = WNLemma.lemmatize(token)\n",
    "#         tag = nltk.pos_tag([token])[0][1]\n",
    "#         if 'NN' in tag and token not in stopWords:\n",
    "#             newTokens.append(token)\n",
    "#     dist = FreqDist(newTokens)\n",
    "#     vocab = dist.keys()\n",
    "#     docs[i] = [w for w in vocab if len(w) > 3 and dist[w] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses indices to modify the array in-place\n",
    "# It removes stopwords before lemmatizing each word\n",
    "\n",
    "# for i in range(len(docs)):\n",
    "#     doc_tokens = docs[i]\n",
    "#     newTokens = []\n",
    "#     for j in range(len(doc_tokens)):\n",
    "#         token = doc_tokens[j]\n",
    "#         if token.istitle():\n",
    "#             token = token.lower()\n",
    "#         doc_tokens[j] = WNLemma.lemmatize(token)\n",
    "#         tag = nltk.pos_tag(token)[0][1]\n",
    "#         if 'NN' in tag:\n",
    "#             newTokens.append(token)\n",
    "#     docs[i] = newTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses indices to modify the array in-place\n",
    "# It removes stopwords before lemmatizing each word\n",
    "\n",
    "# for i in range(len(docs)):\n",
    "#     doc_tokens = docs[i]\n",
    "#     for j in range(len(doc_tokens)):\n",
    "#         token = doc_tokens[j]\n",
    "#         if token.istitle():\n",
    "#             token = token.lower()\n",
    "#         doc_tokens[j] = WNLemma.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses indices to modify the array in-place\n",
    "# It removes stopwords before lemmatizing each word\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    doc_tokens = docs[i]\n",
    "    for j in range(len(doc_tokens)):\n",
    "        token = doc_tokens[j]\n",
    "        if token.istitle():\n",
    "            token = token.lower()\n",
    "        doc_tokens[j] = stemmer.stem(WNLemma.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram = Phrases(docs, min_count=60)\n",
    "# for i in range(len(docs)):\n",
    "#     for token in bigram[docs[i]]:\n",
    "#         if '_' in token:\n",
    "#             docs[i].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19725 unique tokens: ['an', 'and', 'appear', 'ask', 'assess']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=0.1, no_above=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 19714\n",
      "Number of documents: 2385\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set training parameters.\n",
    "# num_topics = 10\n",
    "# chunksize = 1000\n",
    "# passes = 40\n",
    "# iterations = 200\n",
    "# eval_every = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a index to word dictionary.\n",
    "# temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "# id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.ldamodel.LdaModel(\n",
    "#     corpus=corpus,\n",
    "#     id2word=id2word,\n",
    "#     chunksize=chunksize,\n",
    "#     alpha='auto',\n",
    "#     eta='auto',\n",
    "#     iterations=iterations,\n",
    "#     num_topics=num_topics,\n",
    "#     passes=passes,\n",
    "#     eval_every=eval_every\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_topics = model.top_topics(corpus) #, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "# avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "# print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(doc_set)\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [dictionary.doc2bow(doc) for doc in doc_set]\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldaModel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=50)\n",
    "# print(ldaModel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = nltk.probability.FreqDist(docs[4])\n",
    "vocab = dist.keys()\n",
    "freqWords = [w for w in vocab if len(w) > 3 and dist[w] >= 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
